{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dependent-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "typical-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "\n",
    "\n",
    "_atoms = ['He', 'Li', 'Be', 'Ne', 'Na', 'Mg', 'Al', 'Si', 'Cl', 'Ar',\n",
    "          'Ca', 'Ti', 'Cr', 'Fe', 'Ni', 'Cu', 'Ga', 'Ge', 'As', 'Se',\n",
    "          'Br', 'Kr', 'Rb', 'Sr', 'Zr', 'Nb', 'Mo', 'Tc', 'Ru', 'Rh',\n",
    "          'Pd', 'Ag', 'Cd', 'Sb', 'Te', 'Xe', 'Ba', 'La', 'Ce', 'Pr',\n",
    "          'Nd', 'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Er', 'Tm', 'Yb',\n",
    "          'Lu', 'Hf', 'Ta', 'Re', 'Ir', 'Pt', 'Au', 'Hg', 'Tl', 'Pb',\n",
    "          'Bi', 'At', 'Fr', 'Ra', 'Ac', 'Th', 'Pa', 'Pu', 'Am', 'Cm',\n",
    "          'Bk', 'Cf', 'Es', 'Fm', 'Md', 'Lr', 'Rf', 'Db', 'Sg', 'Mt',\n",
    "          'Ds', 'Rg', 'Fl', 'Mc', 'Lv', 'Ts', 'Og']\n",
    "\n",
    "\n",
    "def get_tokenizer_re(atoms):\n",
    "    return re.compile('('+'|'.join(atoms)+r'|\\%\\d\\d|.)')\n",
    "\n",
    "\n",
    "_atoms_re = get_tokenizer_re(_atoms)\n",
    "\n",
    "\n",
    "__i2t = {\n",
    "    0: 'unused', 1: '>', 2: '<', 3: '2', 4: 'F', 5: 'Cl', 6: 'N',\n",
    "    7: '[', 8: '6', 9: 'O', 10: 'c', 11: ']', 12: '#',\n",
    "    13: '=', 14: '3', 15: ')', 16: '4', 17: '-', 18: 'n',\n",
    "    19: 'o', 20: '5', 21: 'H', 22: '(', 23: 'C',\n",
    "    24: '1', 25: 'S', 26: 's', 27: 'Br'\n",
    "}\n",
    "\n",
    "\n",
    "__t2i = {\n",
    "    '>': 1, '<': 2, '2': 3, 'F': 4, 'Cl': 5, 'N': 6, '[': 7, '6': 8,\n",
    "    'O': 9, 'c': 10, ']': 11, '#': 12, '=': 13, '3': 14, ')': 15,\n",
    "    '4': 16, '-': 17, 'n': 18, 'o': 19, '5': 20, 'H': 21, '(': 22,\n",
    "    'C': 23, '1': 24, 'S': 25, 's': 26, 'Br': 27\n",
    "}\n",
    "\n",
    "\n",
    "def smiles_tokenizer(line, atoms=None):\n",
    "    \"\"\"\n",
    "    Tokenizes SMILES string atom-wise using regular expressions. While this\n",
    "    method is fast, it may lead to some mistakes: Sn may be considered as Tin\n",
    "    or as Sulfur with Nitrogen in aromatic cycle. Because of this, you should\n",
    "    specify a set of two-letter atoms explicitly.\n",
    "\n",
    "    Parameters:\n",
    "         atoms: set of two-letter atoms for tokenization\n",
    "    \"\"\"\n",
    "    if atoms is not None:\n",
    "        reg = get_tokenizer_re(atoms)\n",
    "    else:\n",
    "        reg = _atoms_re\n",
    "    return reg.split(line)[1::2]\n",
    "\n",
    "\n",
    "def encode(sm_list, pad_size=50):\n",
    "    \"\"\"\n",
    "    Encodes list of smiles to tensor of tokens\n",
    "    \n",
    "    Args:\n",
    "         sm_list (list) : List of SMILEs strings read from dataset\n",
    "         pad_size (int) : An integer denoting the pad size\n",
    "         \n",
    "    Returns:\n",
    "        tokens (Tensor) : Integer tensor containing indices of tokenized SMILE strings\n",
    "                          of size [len(sm_list), pad_size]\n",
    "        smiles_lens (Tensor) : Integer tensor containing the lengths of tokenized SMILE strings i.e.\n",
    "                        the actual lengths before padding. Size is [len(sm_list)]\n",
    "\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    lens = []\n",
    "    for s in sm_list:\n",
    "        tokens = ([1] + [__t2i[tok]\n",
    "                  for tok in smiles_tokenizer(s)])[:pad_size - 1]\n",
    "        lens.append(len(tokens))\n",
    "        tokens += (pad_size - len(tokens)) * [2]\n",
    "        res.append(tokens)\n",
    "    tokens = torch.tensor(res, dtype=torch.int, device='cuda')\n",
    "    smiles_lens = torch.tensor(lens, dtype=torch.int, device = 'cuda')\n",
    "    return tokens, smiles_lens\n",
    "\n",
    "\n",
    "def decode(tokens_tensor):\n",
    "    \"\"\"\n",
    "    Decodes from tensor of tokens to list of smiles\n",
    "    \"\"\"\n",
    "\n",
    "    smiles_res = []\n",
    "\n",
    "    for i in range(tokens_tensor.shape[0]):\n",
    "        cur_sm = ''\n",
    "        for t in tokens_tensor[i].detach().cpu().numpy():\n",
    "            if t == 2:\n",
    "                break\n",
    "            elif t > 2:\n",
    "                cur_sm += __i2t[t]\n",
    "\n",
    "        smiles_res.append(cur_sm)\n",
    "\n",
    "    return smiles_res\n",
    "\n",
    "\n",
    "def get_vocab_size():\n",
    "    return len(__i2t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sitting-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from gentrl import tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# so the plan here is to have \n",
    "class NewMolecularDataset:\n",
    "    def __init__(self, sources=[], props=['logIC50', 'BFL', 'pipeline'],\n",
    "                 with_missings=False):\n",
    "        self.num_sources = len(sources)\n",
    "        self.source_smiles = []\n",
    "        self.source_props = []\n",
    "        self.source_missings = []\n",
    "        self.source_probs = []\n",
    "\n",
    "        self.with_missings = with_missings\n",
    "\n",
    "        self.len = 0\n",
    "        # this whole for loop is useless because we only have 1 source (i.e)\n",
    "        # only 1 source dict\n",
    "        for source_descr in sources:\n",
    "            cur_df = pd.read_csv(source_descr['path'])\n",
    "            cur_smiles = list(cur_df[source_descr['smiles']].values)\n",
    "            num_smiles = len(cur_smiles)\n",
    "            num_props = len(props)\n",
    "            cur_props = torch.zeros(num_smiles, num_props, device ='cuda') # by default it's float32 tensor\n",
    "            cur_missings = torch.zeros(num_smiles, num_props, dtype=torch.int64, device='cuda')\n",
    "\n",
    "            for i, prop in enumerate(props):\n",
    "                if prop in source_descr:\n",
    "                    if isinstance(source_descr[prop], str):\n",
    "                        cur_props[:, i] = torch.from_numpy(\n",
    "                            cur_df[source_descr[prop]].values)\n",
    "                        # so this is where we read plogp from dataframe\n",
    "                        # and set it to cur_props\n",
    "                        # these are our labels\n",
    "                        # currently cur_props is tensor on CPU\n",
    "                    else:\n",
    "                        cur_props[:, i] = torch.from_numpy(\n",
    "                            cur_df[source_descr['smiles']].map(\n",
    "                                source_descr[prop]).values)\n",
    "                else:\n",
    "                    cur_missings[:, i] = 1\n",
    "            \n",
    "\n",
    "            self.source_smiles.append(cur_smiles)\n",
    "            self.source_props.append(cur_props)\n",
    "            self.source_missings.append(cur_missings)\n",
    "            self.source_probs.append(source_descr['prob'])\n",
    "\n",
    "            self.len = max(self.len, int(num_smiles / source_descr['prob']))\n",
    "\n",
    "        self.source_probs = np.array(self.source_probs).astype(np.float)\n",
    "\n",
    "        self.source_probs /= self.source_probs.sum()\n",
    "        \n",
    "        \n",
    "    def create_gpu_dataset(self):    \n",
    "        trial = np.random.random()\n",
    "\n",
    "        s = 0\n",
    "        # here self.num_sources =1 \n",
    "        for i in range(self.num_sources):\n",
    "            # here self.source_probs = np.array([1])\n",
    "            # so self.source_probs[0] = 1\n",
    "            # so for s = 0 this if condition will always be true\n",
    "            if (trial >= s) and (trial <= s + self.source_probs[i]):\n",
    "                # here bin_len is same as num_smiles\n",
    "                #bin_len = len(self.source_smiles[i])\n",
    "                \n",
    "                # here sm is just idx_th SMILE string\n",
    "                #sm = self.source_smiles[i][idx % bin_len]\n",
    "                sm_list = self.source_smiles[i]\n",
    "                # here self.source_props[0] is cur_props\n",
    "                # so props is the cur_prop value corresponding to \n",
    "                # idx_th SMILE string\n",
    "                # props = self.source_props[i][idx % bin_len]\n",
    "                props = self.source_props[i]\n",
    "                # here self.source_missings[0] is cur_missings\n",
    "                # so miss is the cur_missings value corresponding to \n",
    "                # idx_th SMILE string\n",
    "                # miss = self.source_missings[i][idx % bin_len]\n",
    "                miss = self.source_missings[i]\n",
    "                y = props\n",
    "                if self.with_missings:\n",
    "                    y = torch.concat([props, miss])\n",
    "            # so getitem just returns (idx_th SMILE string, idx_th prop value)\n",
    "            s += self.source_probs[i]\n",
    "        \n",
    "        # so now we have sm_list and their labels (y values)\n",
    "        # we need to tokenize the SMILES string in sm_list and convert the tokens\n",
    "        # to indices\n",
    "        \n",
    "        tokens, smile_lens = encode(sm_list)\n",
    "        \n",
    "        # move the tokens, smile_lens and y tensors to device (CPU or GPU)\n",
    "#         tokens.to('cuda')\n",
    "#         smile_lens.to('cuda')\n",
    "#         y.to('cuda')\n",
    "        \n",
    "        print(\"tokens device\", tokens.device)\n",
    "        print(\"smile_lens device\", smile_lens.device)\n",
    "        print(\"y device\", y.device)\n",
    "        \n",
    "        dataset = TensorDataset(tokens, smile_lens, y)\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "complimentary-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmd = NewMolecularDataset(sources=[\n",
    "    {'path':'train_subset_100_000.csv',\n",
    "     'smiles': 'SMILES',\n",
    "     'prob': 1,\n",
    "     'plogP' : 'plogP',\n",
    "    }], \n",
    "    props=['plogP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "lovely-sperm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens device cuda:0\n",
      "smile_lens device cuda:0\n",
      "y device cuda:0\n"
     ]
    }
   ],
   "source": [
    "gpu_dataset = nmd.create_gpu_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "measured-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "LR = 1e-4\n",
    "NUM_EPOCHS = 1\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "initial-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(gpu_dataset, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, num_workers=NUM_WORKERS,\n",
    "                          pin_memory=PIN_MEMORY, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "professional-harmony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is tensor([[ 1, 23, 23,  ...,  2,  2,  2],\n",
      "        [ 1, 23, 23,  ...,  2,  2,  2],\n",
      "        [ 1, 23, 23,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [ 1,  6, 12,  ...,  2,  2,  2],\n",
      "        [ 1,  9, 13,  ...,  2,  2,  2],\n",
      "        [ 1,  9, 13,  ...,  2,  2,  2]], device='cuda:0', dtype=torch.int32)\n",
      "x shape is torch.Size([50, 50])\n",
      "x device is cuda:0\n",
      "\n",
      "x_lens is tensor([43, 39, 39, 43, 39, 34, 35, 32, 39, 30, 40, 37, 28, 41, 31, 34, 44, 36,\n",
      "        43, 41, 38, 42, 39, 41, 37, 39, 39, 36, 41, 37, 42, 39, 28, 42, 40, 42,\n",
      "        44, 40, 43, 43, 33, 41, 39, 44, 35, 33, 31, 43, 35, 35],\n",
      "       device='cuda:0', dtype=torch.int32)\n",
      "x_lens shape is torch.Size([50])\n",
      "x_lens device is cuda:0\n",
      "\n",
      "y is tensor([[ 0.7073],\n",
      "        [ 1.5643],\n",
      "        [ 0.4647],\n",
      "        [ 0.2846],\n",
      "        [ 1.3440],\n",
      "        [ 1.2395],\n",
      "        [-0.2849],\n",
      "        [ 0.9831],\n",
      "        [ 1.6079],\n",
      "        [ 1.1284],\n",
      "        [ 0.7045],\n",
      "        [ 1.0118],\n",
      "        [ 0.2000],\n",
      "        [-2.8763],\n",
      "        [ 1.3348],\n",
      "        [ 0.3631],\n",
      "        [-1.4349],\n",
      "        [-0.2006],\n",
      "        [-0.1012],\n",
      "        [-3.4325],\n",
      "        [ 0.3952],\n",
      "        [-0.2063],\n",
      "        [ 1.2115],\n",
      "        [-0.0304],\n",
      "        [ 0.1747],\n",
      "        [ 0.5207],\n",
      "        [ 0.9972],\n",
      "        [-1.6823],\n",
      "        [-0.1837],\n",
      "        [ 0.3555],\n",
      "        [-2.3287],\n",
      "        [-1.2797],\n",
      "        [-0.8669],\n",
      "        [ 0.3290],\n",
      "        [-0.6974],\n",
      "        [-0.6581],\n",
      "        [-0.3110],\n",
      "        [ 0.8475],\n",
      "        [ 1.6285],\n",
      "        [-0.4955],\n",
      "        [ 1.2268],\n",
      "        [ 0.6606],\n",
      "        [-0.1697],\n",
      "        [-1.6313],\n",
      "        [ 0.9415],\n",
      "        [-2.6901],\n",
      "        [ 1.2766],\n",
      "        [ 0.5703],\n",
      "        [ 0.8820],\n",
      "        [-0.3540]], device='cuda:0')\n",
      "y shape is torch.Size([50, 1])\n",
      "y device is cuda:0\n"
     ]
    }
   ],
   "source": [
    "for x, x_lens, y in train_loader:\n",
    "    print(f\"x is {x}\")\n",
    "    print(f\"x shape is {x.shape}\")\n",
    "    print(f\"x device is {x.device}\")\n",
    "    print()\n",
    "    print(f\"x_lens is {x_lens}\")\n",
    "    print(f\"x_lens shape is {x_lens.shape}\")\n",
    "    print(f\"x_lens device is {x_lens.device}\")\n",
    "    print()\n",
    "    print(f\"y is {y}\")\n",
    "    print(f\"y shape is {y.shape}\")\n",
    "    print(f\"y device is {y.device}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "turned-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainStats():\n",
    "    def __init__(self):\n",
    "        self.stats = dict()\n",
    "\n",
    "    def update(self, delta):\n",
    "        for key in delta.keys():\n",
    "            if key in self.stats.keys():\n",
    "                self.stats[key].append(delta[key])\n",
    "            else:\n",
    "                self.stats[key] = [delta[key]]\n",
    "\n",
    "    def reset(self):\n",
    "        for key in self.stats.keys():\n",
    "            self.stats[key] = []\n",
    "\n",
    "    def print(self):\n",
    "        for key in self.stats.keys():\n",
    "            avg = sum(self.stats[key]) / len(self.stats[key])\n",
    "            print(str(key) + \": {:4.4};\".format(avg), end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "seeing-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo = torch.tensor(31, device='cuda')\n",
    "rec_part = torch.tensor(31, device='cuda')\n",
    "kldiv_part = torch.tensor(31, device='cuda')\n",
    "log_p_y_by_z = torch.rand(31, device='cuda')\n",
    "log_p_z_by_y = torch.rand(31, device='cuda')\n",
    "cur_stats = {\n",
    "            'loss': -elbo,\n",
    "            'rec': rec_part,\n",
    "            'kl': kldiv_part,\n",
    "            'log_p_y_by_z': log_p_y_by_z.mean(),\n",
    "            'log_p_z_by_y': log_p_z_by_y.mean()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "induced-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_stats = TrainStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "divine-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_stats.update(cur_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "perfect-dakota",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [tensor(-3, device='cuda:0'), tensor(-31, device='cuda:0')],\n",
       " 'rec': [tensor(3, device='cuda:0'), tensor(31, device='cuda:0')],\n",
       " 'kl': [tensor(3, device='cuda:0'), tensor(31, device='cuda:0')],\n",
       " 'log_p_y_by_z': [tensor(0.5874, device='cuda:0'),\n",
       "  tensor(0.4797, device='cuda:0')],\n",
       " 'log_p_z_by_y': [tensor(0.2277, device='cuda:0'),\n",
       "  tensor(0.5117, device='cuda:0')]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_stats.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "small-alberta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -17.0;rec: 17.0;kl: 17.0;log_p_y_by_z: 0.5335;log_p_z_by_y: 0.3697;\n"
     ]
    }
   ],
   "source": [
    "local_stats.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-recycling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
